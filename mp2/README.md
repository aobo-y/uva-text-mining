# Machine Problem 2

## Part1: Parameter Estimation

### 1.1

Implementation of maximum likelihood estimation, where
- `pre_token` is the previous tag and `token` is the current tag in transition probability
-  `pre_token` is the hidden tag and `token` is the word in emission probability
- a `cache` is used to store the calculated results on-the-fly to avoid redundancy

```python
def calc_prob(self, pre_token, token):
  ''' Additive smoothed probability '''

  counts = self.counts[pre_token]

  if pre_token not in self.cache:
    self.cache[pre_token] = {
      'sum': sum(counts.values()) + self.delta * len(self.tokens),
      'probs': {}
    }

  cache = self.cache[pre_token]

  if token in cache['probs']:
    return cache['probs'][token]

  val = counts[token] if token in counts else 0
  val = (val + self.delta) / cache['sum']

  cache['probs'][token] = val

  return val
```

### 1.2

Top 10 probable words under tag `NN` and tags after tag `VB`

idx |Words under `NN`|Tags after `VB`
-|-|-
1 | % | DT
2 | company | IN
3 | year | VBN
4 | market | JJ
5 | trading | NN
6 | stock | NNS
7 | program | PRP$
8 | president | RB
9 | share | TO
10 | government | PRP

## Part2: Viterbi Algorithm for Posterior Inference

### 2.1

Implementation of the Viterbi algorithm for HMM

```python
def viterbi(trs_mdl, ems_mdl, seqs):
  '''
  Inputs:
    trs_mdl: transition language model
    ems_mdl: emission language model
    seqs: sequence of tokens
  Outputs:
    hseqs: hidden state sequence
    score: corresponding log likelihood
  '''

  hstates = trs_mdl.tokens # hidden states

  v_scores = [{t: -inf for t in hstates} for _ in range(len(seqs))]
  b_hstates = [{t: None for t in hstates} for _ in range(len(seqs))]

  for idx, token in enumerate(seqs):
    pre_states = hstates if idx > 0 else ['START']

    for state in hstates:
      for pre_state in pre_states:
        pre_score = v_scores[idx - 1][pre_state] if idx > 0 else 0

        trs_score = log(trs_mdl.calc_prob(pre_state, state))
        ems_score = log(ems_mdl.calc_prob(state, token))

        new_score = pre_score + trs_score + ems_score
        if new_score > v_scores[idx][state]:
          v_scores[idx][state] = new_score
          b_hstates[idx][state] = pre_state

  max_score = -inf
  last_state = None

  for state, score in v_scores[len(seqs) - 1].items():
    if score > max_score:
      max_score = score
      last_state = state

  hseqs = [None] * len(seqs)
  hseqs[-1] = last_state
  for i in range(len(seqs) - 1, 0, -1):
    hseqs[i - 1] = b_hstates[i][hseqs[i]]

  return hseqs, max_score
```

### 2.2

The overall metrics of 5-fold cross-validation

metric | value
-|-
accuracy | 0.9076
precisions | 0.78
recalls | 0.7671

For each tag

tag | precision | recall
-|-|-
NN | 0.9111 | 0.8697
VB | 0.868 | 0.9174
JJ | 0.8431 | 0.8076
NNP | 0.8952 | 0.8257

### 2.3

Setting the smoothing variables delta to `3` and sigma to `0.04` achieves the highest accuracy, `0.9133`, with decent precision `0.7642` and recall `0.7897`.

## Part3: Generate Sentence via an HMM

### 3.1

Implementation of the sampling, where `pre_token` is the previous tag in transition sampling and the hidden tag in emission sampling

```python
def sampling(self, pre_token):
  prob = D(random.random())

  for token in self.tokens:
    token_prob = self.calc_prob(pre_token, token)
    prob -= token_prob
    if prob < 0:
      return token, token_prob

  raise Exception('Failed to sample: Out of token')
```

### 3.2

Top 10 sentences with the highest log-likelihood

```
the way , the most in the first market did
for the tire of the patent to five 2 program
the net director says the members for the talks ...
with replacement years , about be to buy and they
the commerce northeast , visible ventures , the problem for
'' the to be that five-point have according to us$
in this available members of lap-shoulder leaders with the corporate
the device and program-trading , random that october , of
however blamed that risk-free , but value , have to
the yesterday is here on the history % program nasdaq
```

They are a little more natural than the sentences generated by the unigram model, but are NOT more natural/readable than the bigram model. Because sampling sentences in this way, we take the POS relationship into consideration, which is an improvement compared with the unigram. However, the bigram directly encodes word-to-word relations into the model, which I believe contains more than the hidden POS information.

### 3.3

The average tagging accuracy of the generated sentences is about `0.85`.

The sentence-level tagging accuracy and the log-likelihood of each sentence are in Positive Correlation, which means the sentences with higher log-likelihood usually have higher accuracy. It is because Viterbi for HMM produces the hidden tags with the highest probability, or log-likehood. Therefore, the predicted tags have more in common with the sentences of high log-likelihood, which leads to high accuracy.




